{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re going to demonstrate how you might draw samples from the multivariate Gaussian density given by:\n",
    "\n",
    "p(x⃗ )∝exp[−12(x⃗ −μ⃗ )TΣ−1(x⃗ −μ⃗ )]\n",
    "where μ⃗  is an N-dimensional vector position of the mean of the density and Σ is the square N-by-N covariance matrix.\n",
    "Then, we’ll code up a Python function that returns the density p(x⃗ ) for specific values of x⃗ , μ⃗  and Σ−1. In fact, emcee actually requires the logarithm of p. We’ll call it log_prob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob(x, mu, cov):\n",
    "    diff = x - mu\n",
    "    return -0.5 * np.dot(diff, np.linalg.solve(cov, diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important that the first argument of the probability function is the position of a single “walker” (a N dimensional numpy array). The following arguments are going to be constant every time the function is called and the values come from the args parameter of our EnsembleSampler that we’ll see soon.\n",
    "\n",
    "Now, we’ll set up the specific values of those “hyperparameters” in 5 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 5\n",
    "\n",
    "np.random.seed(42)\n",
    "means = np.random.rand(ndim)\n",
    "\n",
    "cov = 0.5 - np.random.rand(ndim ** 2).reshape((ndim, ndim))\n",
    "cov = np.triu(cov)\n",
    "cov += cov.T - np.diag(cov.diagonal())\n",
    "cov = np.dot(cov, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we use 32 walkers? Before we go on, we need to guess a starting point for each of the 32 walkers. This position will be a 5-dimensional vector so the initial guess should be a 32-by-5 array. It’s not a very good guess but we’ll just guess a random number between 0 and 1 for each component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwalkers = 32\n",
    "\n",
    "p0 = np.random.rand(nwalkers, ndim)\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob, args=[means, cov])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve gotten past all the bookkeeping stuff, we can move on to the fun stuff. The main interface provided by emcee is the EnsembleSampler object so let’s get ourselves one of those.\n",
    "Remember how our function log_prob required two extra arguments when it was called? By setting up our sampler with the args argument, we’re saying that the probability function should be called as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob(p0[0], means, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = sampler.run_mcmc(p0, 100)\n",
    "sampler.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll notice that I saved the final position of the walkers (after the 100 steps) to a variable called state. You can check out what will be contained in the other output variables by looking at the documentation for the EnsembleSampler.run_mcmc() function. The call to the EnsembleSampler.reset() method clears all of the important bookkeeping parameters in the sampler so that we get a fresh start. It also clears the current positions of the walkers so it’s a good thing that we saved them first.\n",
    "\n",
    "Now, we can do our production run of 10000 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.run_mcmc(state, 10000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples can be accessed using the EnsembleSampler.get_chain() method. This will return an array with the shape (10000, 32, 5) giving the parameter values for each walker at each step in the chain. Take note of that shape and make sure that you know where each of those numbers come from. You can make histograms of these samples to get an estimate of the density that you were sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.get_chain(flat=True)\n",
    "plt.hist(samples[:, 0], 100, color=\"k\", histtype=\"step\")\n",
    "plt.xlabel(r\"$\\theta_1$\")\n",
    "plt.ylabel(r\"$p(\\theta_1)$\")\n",
    "plt.gca().set_yticks([]);\n",
    "\n",
    "print(\"Mean acceptance fraction: {0:.3f}\".format(np.mean(sampler.acceptance_fraction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The generative probabilistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# Choose the \"true\" parameters.\n",
    "m_true = -0.9594\n",
    "b_true = 4.294\n",
    "f_true = 0.534\n",
    "\n",
    "# Generate some synthetic data from the model.\n",
    "N = 50\n",
    "x = np.sort(10 * np.random.rand(N))\n",
    "yerr = 0.1 + 0.5 * np.random.rand(N)\n",
    "y = m_true * x + b_true\n",
    "y += np.abs(f_true * y) * np.random.randn(N)\n",
    "y += yerr * np.random.randn(N)\n",
    "\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "x0 = np.linspace(0, 10, 500)\n",
    "plt.plot(x0, m_true * x0 + b_true, \"k\", alpha=0.3, lw=3)\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he linear least squares solution to these data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.vander(x, 2)\n",
    "C = np.diag(yerr * yerr)\n",
    "ATA = np.dot(A.T, A / (yerr ** 2)[:, None])\n",
    "cov = np.linalg.inv(ATA)\n",
    "w = np.linalg.solve(ATA, np.dot(A.T, y / yerr ** 2))\n",
    "print(\"Least-squares estimates:\")\n",
    "print(\"m = {0:.3f} ± {1:.3f}\".format(w[0], np.sqrt(cov[0, 0])))\n",
    "print(\"b = {0:.3f} ± {1:.3f}\".format(w[1], np.sqrt(cov[1, 1])))\n",
    "\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "plt.plot(x0, m_true * x0 + b_true, \"k\", alpha=0.3, lw=3, label=\"truth\")\n",
    "plt.plot(x0, np.dot(np.vander(x0, 2), w), \"--k\", label=\"LS\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical notation, the correct likelihood function is:\n",
    "\n",
    "lnp(y|x,σ,m,b,f)=−12∑n[(yn−mxn−b)2s2n+ln(2πs2n)]\n",
    "where\n",
    "\n",
    "s2n=σ2n+f2(mxn+b)2.\n",
    "This likelihood function is simply a Gaussian where the variance is underestimated by some fractional amount: f. In Python, you would code this up as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y, yerr):\n",
    "    m, b, log_f = theta\n",
    "    model = m * x + b\n",
    "    sigma2 = yerr ** 2 + model ** 2 * np.exp(2 * log_f)\n",
    "    return -0.5 * np.sum((y - model) ** 2 / sigma2 + np.log(sigma2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #A good way of finding this numerical optimum of this likelihood function is to use the scipy.optimize module:\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "np.random.seed(42)\n",
    "nll = lambda *args: -log_likelihood(*args)\n",
    "initial = np.array([m_true, b_true, np.log(f_true)]) + 0.1 * np.random.randn(3)\n",
    "soln = minimize(nll, initial, args=(x, y, yerr))\n",
    "m_ml, b_ml, log_f_ml = soln.x\n",
    "\n",
    "print(\"Maximum likelihood estimates:\")\n",
    "print(\"m = {0:.3f}\".format(m_ml))\n",
    "print(\"b = {0:.3f}\".format(b_ml))\n",
    "print(\"f = {0:.3f}\".format(np.exp(log_f_ml)))\n",
    "\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "plt.plot(x0, m_true * x0 + b_true, \"k\", alpha=0.3, lw=3, label=\"truth\")\n",
    "plt.plot(x0, np.dot(np.vander(x0, 2), w), \"--k\", label=\"LS\")\n",
    "plt.plot(x0, np.dot(np.vander(x0, 2), [m_ml, b_ml]), \":k\", label=\"ML\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
